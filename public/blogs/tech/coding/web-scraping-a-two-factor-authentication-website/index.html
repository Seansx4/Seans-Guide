<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Web Scraping a Two-Factor Authentication Website</title>
    <meta name="description" content="This blog outlines how to correctly web scrape a dynamic website protected with 2FA using Python and Selenium." />
    <link rel="stylesheet" href="/css/styles.css" />
    <script src="https://identity.netlify.com/v1/netlify-identity-widget.js"></script>
    <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-E6SGQCPF41"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-E6SGQCPF41');
</script>

  </head>
  <body>
    <header class="header">
  <nav class="nav collapsible">
    <span class="nav__item">
      <a class="nav__brand" href="/">
        <img src="/images/logo.jpg" alt="Brand logo" />
      </a>
    </span>

    <svg class="icon icon--white nav__toggler">
      <use href="/images/sprite.svg#menu"></use>
    </svg>
    <ul class="list nav__list collapsible__content">
      <li class="nav__item"><a href="/">Home</a></li>
      <li class="nav__item"><a href="/blog/">Blog</a></li>
      <li class="nav__item"><a href="/contact/">Contact</a></li>
    </ul>
  </nav>
</header>

    <main>
      
<section class="block">
  <div class="container">
    <header class="block__header" data-aos="fade-up">
      <h1 class="blog--header">Web Scraping Content from Two-Factor Authentication Website</h1>
    </header>

    <article class="feature grid grid--1x2" data-aos="zoom-in-up">
      <div class="feature__content">
         <p>Web scraping is a fantastic tool for any programmer to add to their skill
  set. Web scraping allows you to automate data retrieval from different websites. For example, you could scrape a list of email addresses from one website, the price
  of products from another, or even the Google search results page for specific
  search queries. The possibilities are endless, and the process is both helpful
  and fun to set up.</p>


  <h3>Jump to:<h3>

  <ul class="list--jump">

  <li><a class="jump" href="#2FA">Why Scrape a Website with 2FA-Factor Authentication?</a></li>

  <li><a class="jump" href="#StaticVsDynamic">Static Vs. Dynamic Websites</a></li>
  <li><a class="jump" href="#ScrapingDynamicSelenium
">Scraping a Dynamic Website with Selenium</a></li>

  <li><a class="jump" href="#step1">Step One: Simulating the Process</a></li>

  <li><a class="jump" href="#step2">Step Two: Creating the Scraper</a></li>
  <li><a class="jump" href="#step3">Step Three: Bypassing Two-Factor Authentication</a></li>
  <li><a class="jump" href="#step4">Step Four: Using Excel to Tidy Data</a></li>
  <li><a class="jump" href="#step5">Step Five: Scraping Content Page URLs</a></li>


  <li><a class="jump" href="#step6">Creating a Headless Browser</a></li>
  <li><a class="jump" href="#fullcode">Full Code</a></li>

  </ul>
      </div>
      <div class="feature__order">
        <div class="click-zoom">
          <label>
            <input type="checkbox" />
            <img
              class="feature__image image--rounded"
              src="/images/code.jpg"
              alt="Python code for web scraping"
            />
          </label>
        </div>
      </div>
    </article>

    
    <article class="feature">
      <div class="feature__content">
        

        <h2 id="2FA">Why Scrape a Website with 2FA-Factor Authentication?</h2>

  <p>The need to scrape a website with two-factor authentication (2FA) is quite rare. 2FA provides an extra layer of security to a user account of a given platform. When a user attempts to login to the platform a once off code is often sent to the email address associated with the account. Typically, most data the user can access on their dashboard can be exported, or there is usually an API for data retrieval. If neither of these options exist however, webscraping can allow you to retrieve the data you need, you just have the small task of bypassing 2FA.</p>

<p>When attempting to scrape a 2FA protected website myself, I found various tutorials outlining how to login to a regular website, but very sparse information on how to circumnavigate 2FA. I should make it clear that this tutorial is for someone who is the true owner of an account protected with 2FA and not a guide on how to hack someone else’s account. </p>

<p>I like to keep my blogs focused on the topic of discussion, so I won’t be going through the technicalities of how to install the required dependencies (Selenium and the WebDriver) in this blog, or any basic concepts of Python programming. In the future I am going to write some content surrounding this but for now I will leave it up to you to do some quick research (I recommend YouTube). </p>

<h2 id="StaticVsDynamic">Static vs Dynamic Websites?</h2>

  <p>There are two possible options to take when trying to overcomethe scraping a 2FA website, but which option you chose depends largely on what kind of website you are trying to scrape. If you are unsure of what kind of website you are scraping, check out my guide Static vs Dynamic Sites: What’s the Difference? </p>


  <p>If your site is a static website you can simply use the beautifulsoup library in conjunction with a neat little trick to store your cookies (I will be writing some content on this soon). This allows you to bypass login and TFA and scrape the data as required. If you are scraping a dynamic website however, thing can be a little more complicated. </p>


<h2 id="ScrapingDynamicSelenium">Scraping a Dynamic Website with Selenium</h2>
  <p>Below I have outlined how to use Selenium to scrape the user dashboard of a BigCommerce website. Selenium allows for the automation of a webdriver, thereby simulating a web browsers interaction with a web page. BigCommerce is a popular eCommerce provider. The user dashboard for a BigCommerce website is dynamically generated and protected by 2FA at login. While this example will undoubtedly be different to your problem, changing some lines of code or following the general methodology may help you solve your problem.</p>


  <p>For this example, we need to fetch the URL for each of our content pages (this was my original problem as BigCommerce does not allow the export of your content page URLs). Below is the list of our content pages once we have logged into our dashboard.</p>
      </div>
    </article>
    
    <div class="blog__image__container">
      <div class="click-zoom">
        <label>
          <input type="checkbox" />
          <img
            class="feature__image"
            src="/images/contentPage.png"
            alt="Content page with URLs"
          />
        </label>
      </div>
      
      <p>BigCommerce page with the list of URLs</p>
      
    </div>
      
    <article class="feature">
      <div class="feature__content">
        

        <p>While we can see what looks like links to our content pages, if we right click
  and inspect on any given link we can see that it is actually a link to another
  page. </p>

      </div>
    </article>
    
    <div class="blog__image__container">
      <div class="click-zoom">
        <label>
          <input type="checkbox" />
          <img
            class="feature__image"
            src="/images/inspectingContentpage.png"
            alt="Inspecting URLs"
          />
        </label>
      </div>
      
      <p>Inspecting URLss</p>
      
    </div>
      
    <article class="feature">
      <div class="feature__content">
         <p>This next page acts as an editorial page for the content page, and it
  is here that the content web page URL is stored.</p>
      </div>
    </article>
    
    <div class="blog__image__container">
      <div class="click-zoom">
        <label>
          <input type="checkbox" />
          <img
            class="feature__image"
            src="/images/editorialPage.png"
            alt="Editorial page"
          />
        </label>
      </div>
      
      <p>Editorial page, note you can see the correct web page URL.</p>
      
    </div>
      
    <article class="feature">
      <div class="feature__content">
           <p>This means that our task is split into two stages. First, we must scrape
  the list of editorial URLs on the dashboard, before visiting each URL
  individually and scraping the required content web page URL. </p>



  <h2 id="step1">Step One: Simulating the Process</h2>

  <p>The first step of our web scraping journey is to attempt to access our BigCommerce dashboard. To do this we must login and pass our one-time 2FA code for login. To simulate the process, we are going to login to the dashboard from an incognito browser. This ensures we follow the same route our program will, and that we are not automatically logged in.</p>
      </div>
    </article>
    
    <div class="blog__image__container">
      <div class="click-zoom">
        <label>
          <input type="checkbox" />
          <img
            class="feature__image"
            src="/images/login.png"
            alt="Login"
          />
        </label>
      </div>
      
    </div>
      
    <article class="feature">
      <div class="feature__content">
         <p>By inspecting the email field, we can see the code for the input. In this example we can see that the input field has name = “user[email]” and id = “user_email”. We can target either of these within our webscraper but in this case I am going to use the name attribute for the sake of consistency, as the login button has a name but not an ID. </p>
      </div>
    </article>
    
    <div class="blog__image__container">
      <div class="click-zoom">
        <label>
          <input type="checkbox" />
          <img
            class="feature__image"
            src="/images/inspectingemailinput.png"
            alt="Inspecting email input"
          />
        </label>
      </div>
      
      <p>Using developer tools we inspect the email input field. We then repeat this for the password field and the login button to find their names.</p>
      
    </div>
      
    <article class="feature">
      <div class="feature__content">
          <p>We then repeat this process for the password. And again for the login
  button. </p>

<p>From this we can see that the password has a name=“user[password]” and the login button has a name=“commit”.</p>
<p>After logging in manually we are redirected towards a new page requesting our 2FA code. On this page we carry out the same inspection process to find the name of the code field and the verify button.</p>
      </div>
    </article>
    
    <div class="blog__image__container">
      <div class="click-zoom">
        <label>
          <input type="checkbox" />
          <img
            class="feature__image"
            src="/images/inspectingverify.png"
            alt="Inspecting the verify option"
          />
        </label>
      </div>
      
    </div>
      
    <article class="feature">
      <div class="feature__content">
         <p>In this instance they have the attributes name=“verification[opt_code]” and name=“commit”.</p>

<h2 id="step2">Step Two: Creating the Scraper</h2>
<p>With this data we can now begin to write our python program. Below are the lines of code for the dependencies you will need:</p>

<pre class="code_terminal">
          <code>
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
import time

          </code>
        </pre>

 <p>Next we initialise our chrome driver for the actual scraping. This is
  achieved by storing the browser driver file path in a variable and then using that
  variable to instantiate a driver object. The chrome service is to avoid
  depreciation in future Selenium releases.</p>

<pre class="code_terminal">
          <code>
chrome_options = Options()

path = Service("C:\Program Files (x86)\chromedriver.exe")

driver = webdriver.Chrome(service=path)
 </code>
        </pre>


<p>Next we create some variables to store our user email, password, dashboard URL and the URL for the page containing all of our content pages, in this case I have named it content.</p>

<pre class="code_terminal">
          <code>
username = "useremail@mail.com"
password = "userpassword1"

dashboard = "https://store.mybigcommerce.com/manage/dashboard"
content = "https://store.mybigcommerce.com/manage/content/pages"
 </code>
        </pre>

<p>We then pass the dashboard URL to the driver. When the driver attempts to access the dashboard page it will automatically be redirected to the login page instead. This is where our variables come into play.</p>
<p>We use the “.find_element” function to find the user input fields which we earlier identified. We then use the “.send_keys” method and pass our matching variables, so username is passed into the email input and password is passed into the password input. We then locate the login button by its name of “commit”, and use .click() to continue.</p>

<pre class="code_terminal">
          <code>
driver.get(dashboard)

driver.find_element(By.NAME, "user[email]").send_keys(username)
driver.find_element(By.NAME, "user[password]").send_keys(password)
driver.find_element(By.NAME, "commit").click()

 </code>
        </pre>

<h2 id="step3">Step Three: Bypassing Two-Factor Authentication</h2>
<p>Next we create a variable called “verification” which stores a run time user input. This step allows the program to pause as it waits for our input and is essentially the step that allows us to bypass 2FA. During this pause we check our emails for the verification code, paste it into the terminal and hit enter. Be sure not to include any spaces after the code or else the program will crash. Once we hit enter our program continues from where it left off. At this point it finds the input field for the verification code. Using “.send_keys” we send the new “verification” variable to the input field before using “.click()” to verify the code.</p>


<pre class="code_terminal">
          <code>
verification = input("Enter verification code: ")

driver.find_element(By.NAME,device_verification[otp_code]").send_keys(verification)

driver.find_element(By.NAME, "commit").click()


 </code>
        </pre>

<p>Just like that we are into the BigCommerce dashboard past the 2FA! At this stage we can navigate directly to the URL stored in our content variable. This “content” page contains all of the links to our editorial pages, which in turn store our desired URLs. In this example I had to switch iframes to the iframe containing the content we require. To find the correct iframe simply use developer tools to inspect the content you are attempting to scrape, use CTRL + F to search for the keyword "iframe" and take the name of the iframe that your content is contained within. This step is not always required. </p>

<pre class="code_terminal">
          <code>
driver.get(content)
driver.switch_to.frame("content-iframe")



 </code>
        </pre>

<p>Inspecting each page link we can see that it is contained within an <a> tag that has a title of “Edit this page”, and so it is this that we will target in our scraping bot. </p>



      </div>
    </article>
    
    <div class="blog__image__container">
      <div class="click-zoom">
        <label>
          <input type="checkbox" />
          <img
            class="feature__image"
            src="/images/contentPage.png"
            alt="Inspecting URL"
          />
        </label>
      </div>
      
    </div>
      
    <article class="feature">
      <div class="feature__content">
         <p>To scrape all of these editorial page links we create a for loop, instructing the driver to find all <a> elements with the title of “Edit this page”. The for loop then cycles through all of these <a> tags and prints their hrefs, which is their editorial links. The code is below.</p>

<pre class="code_terminal">
          <code>
for result in driver.find_elements(By.CSS_SELECTOR, 'a[title="Edit this page"]'):
    print(result.get_attribute("href"))




 </code>
        </pre>
Once we run this program we get a number of URLs printed on our terminal. 
      </div>
    </article>
    
    <div class="blog__image__container">
      <div class="click-zoom">
        <label>
          <input type="checkbox" />
          <img
            class="feature__image"
            src="/images/codeterminal.png"
            alt="code terminal of URLs"
          />
        </label>
      </div>
      
    </div>
      
    <article class="feature">
      <div class="feature__content">
         <h2 id="step4">Using Excel to Tidy Our Data:</h2>
<p>Once we have a list of editorial page URLs we can move onto stage two of the process. First, we simply copy and paste the terminal code into Excel to tidy up the data. We can remove any duplicates, filter for URLs not containing “pageId=” and delete them, and prepare the new structure for storing this list of URLs in a Python list. This stage could be handled as part of the Python program itself, but sometimes I find Excel easier to visualize the data. </p>
      </div>
    </article>
    
    <div class="blog__image__container">
      <div class="click-zoom">
        <label>
          <input type="checkbox" />
          <img
            class="feature__image"
            src="/images/excelurls.png"
            alt="list of URLs in Excel"
          />
        </label>
      </div>
      
    </div>
      
    <article class="feature">
      <div class="feature__content">
         <p>We can prepare the URL structure by using the concatenate formula. The correct URL structure is as follows:</p>
<p>“URL“, &rarr; which looks like &rarr; “example.com/example-page”,</p>
<p>To do this we add a “ “, to the cells on either side of our URLs. We then use the concatenate formula to join them to our URLs. Note the dollar signs in the cells on either side of the URL. This is to lock these cells in place meaning, the only cell to change as we go down the sheet is the URL. This is achieved by pressing F4.</p>
      </div>
    </article>
    
    <div class="blog__image__container">
      <div class="click-zoom">
        <label>
          <input type="checkbox" />
          <img
            class="feature__image"
            src="/images/concatenate.png"
            alt="Using the concatenate formula in Excel"
          />
        </label>
      </div>
      
    </div>
      
    <article class="feature">
      <div class="feature__content">
         <p>By dragging down we can now apply the correct structure to all of our URLs.</p>
      </div>
    </article>
    
    <div class="blog__image__container">
      <div class="click-zoom">
        <label>
          <input type="checkbox" />
          <img
            class="feature__image"
            src="/images/correcturls.png"
            alt="Correct URL structure in Excel"
          />
        </label>
      </div>
      
    </div>
      
    <article class="feature">
      <div class="feature__content">
         <p>We can now take the list of URLs and add them to a list in our Python program.</p>

<pre class="code_terminal">
          <code>
urls = ["https://store-h68l9z2lnx.mybigcommerce.com/admin/index.php?ToDo=editPage&pageId=1519",
"https://store-h68l9z2lnx.mybigcommerce.com/admin/index.php?ToDo=editPage&pageId=12542",
"https://store-h68l9z2lnx.mybigcommerce.com/admin/index.php?ToDo=editPage&pageId=1538",
"https://store-h68l9z2lnx.mybigcommerce.com/admin/index.php?ToDo=editPage&pageId=1540",
"https://store-h68l9z2lnx.mybigcommerce.com/admin/index.php?ToDo=editPage&pageId=1786",
]





 </code>
        </pre>
<h2 id="step5">Scraping Content Page URLs:</h2>
<p>Once we have this list of URLs which contains the actual data we require, we can use a for loop to cycle through them. Applying a wait of 3 seconds to the browser helps to ensure the page has finished loading and we are not missing any elements. </p>

<p>Next we can then create our desired output URL by creating a string in which we concatenate our domain name to the individual page URL. We locate the individual page URL by using its ID after inspecting.</p>
      </div>
    </article>
    
    <div class="blog__image__container">
      <div class="click-zoom">
        <label>
          <input type="checkbox" />
          <img
            class="feature__image"
            src="/images/customurl.png"
            alt="Retrieving the custom URL for the webpage"
          />
        </label>
      </div>
      
    </div>
      
    <article class="feature">
      <div class="feature__content">
         <p>We then print the full URL for the page before moving on to the next URL in the list. </p>

<pre class="code_terminal">
          <code>
for url in urls:
    driver.get(url)
    time.sleep(3)
    driver.switch_to.frame("content-iframe")
    url = "https://www.example.com" + driver.find_element(By.ID, "page_custom_url").get_attribute("value")
    print(url)






 </code>
        </pre>

<h2 id="step6">Creating a Headless Browser:</h2>

<p>The final piece to the puzzle is to turn this session into a headless session, meaning selenium will run in the background for us and we do not have to watch the browser. To do this we simply create a chrome options object, in which we set the browser option to headless. We then pass this options object as an argument to our driver.</p><pre class="code_terminal">
          <code>
chrome_options = Options()
chrome_options.headless = True
driver = webdriver.Chrome(service=path, options=chrome_options)







 </code>
 </pre>
      </div>
    </article>
      
  </div>
</section>

<section data-aos="fade-up" class="block container">
  <header class="block__header">
    <h3>Blog by Category</h3>
  </header>

  <div class="center">
  <ul class="center_list">
    <li>
      <a href="/featured-blogs/">Feautured</a>
    </li>
    <li>
      <a href="/coding-blogs/">Coding</a>
    </li>
    <!-- <li>
      <a href="/DIY-blogs/">DIY</a>
    </li>
    <li>
      <a href="/outdoors-blogs/">Outdoors</a>
    </li> -->
  </ul>
</div>

</section>

<script src="js/main.js"></script>
<script src="https://unpkg.com/aos@next/dist/aos.js"></script>
<script>
  AOS.init();
</script>

    </main>
    <script src="https://unpkg.com/aos@next/dist/aos.js"></script>
<script>
  AOS.init();
</script>
    <footer><footer class="block block--dark footer">
  <div class="container grid footer__sections">
    <section class="collapsible collapsible--expanded footer__section">
      <header class="collapsible__header">
        <h2 class="collapsible__heading footer__heading">Sean's Guide</h2>
        <svg class="icon icon--white collapsible-icon">
          <use href="images/sprite.svg#chevron"></use>
        </svg>
      </header>
      <div class="collapsible__content">
        <ul class="list">
          <li><a href="/">Home</a></li>
          <li><a href="/blog/">Blog</a></li>
          <li><a href="/contact/">Contact</a></li>
        </ul>
      </div>
    </section>

    <section class="footer__brand">
      <a class="nav__brand" href="index.html">
        <img src="/images/logo2.jpg" alt="Brand logo"
      /></a>
      <p class="footer__year">2022</p>
    </section>
  </div>
</footer>
</footer>
    <script src="/js/main.js"></script>
    <script>
      if (window.netlifyIdentity) {
        window.netlifyIdentity.on("init", (user) => {
          if (!user) {
            window.netlifyIdentity.on("login", () => {
              document.location.href = "/admin/";
            });
          }
        });
      }
    </script>
  </body>
</html>

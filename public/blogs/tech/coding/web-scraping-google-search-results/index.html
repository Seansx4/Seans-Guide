<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Web Scraping Google Search Results</title>
    <meta name="description" content="In this article I focus on web scraping the Google Search Results Page for a search term of interest. This simple approach utilises Python and Beautiful Soup and may be used for tracking the position of a page or domain in Google for a given search term. " />
    <link rel="stylesheet" href="/css/styles.css" />
    <script src="https://identity.netlify.com/v1/netlify-identity-widget.js"></script>
    <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-E6SGQCPF41"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-E6SGQCPF41');
</script>

  </head>
  <body>
    <header class="header">
  <nav class="nav collapsible">
    <span class="nav__item">
      <a class="nav__brand" href="/">
        <img src="/images/logo.jpg" alt="Brand logo" />
      </a>
    </span>

    <svg class="icon icon--white nav__toggler">
      <use href="/images/sprite.svg#menu"></use>
    </svg>
    <ul class="list nav__list collapsible__content">
      <li class="nav__item"><a href="/">Home</a></li>
      <li class="nav__item"><a href="/blog/">Blog</a></li>
      <li class="nav__item"><a href="/contact/">Contact</a></li>
    </ul>
  </nav>
</header>

    <main>
      


<!-- JSON-LD markup generated by Google Structured Data Markup Helper. -->
<script type="application/ld+json">

  

\
{ "@context" : "http://schema.org", "@type" : "Article", "name" : "Web Scraping Google Search Results", "author" : { "@type" : "Person", "name" : "Sean Grayson" }, "datePublished" : "2022-06-26", "image" : "", "url" : "" }
</script>

<section class="block">
  <div class="container">
    <header class="block__header" data-aos="fade-up">
      <h1 class="blog--header">Web Scraping Google Search Results</h1>
    </header>

    <article class="feature grid grid--1x2" data-aos="zoom-in-up">
      <div class="feature__content">
        <h2>Step 1: Search Query, Location and Number of Results</h2>



<p>The first step to web scraping the Google Search Results Page is to ensure we use the correct URL structure. The URL structure of Google Search looks like the following: “https://www.google.com/search?q=”plus our search term, with spaces replaced by “+”. So, for the search of “garden hose” the URL would look like “https://www.google.com/search?q=garden+hose”. </p>

<p>Next, we must determine the location we want to see the results for. This is achieved by adding a "location" parameter to our URL after our search query. You can see in the screenshot below I have added “&gl=us” to the end of the URL after I search for the term “garden hose” to set the results to that of Google US.</p>
      </div>
      <div class="feature__order">
        <div class="click-zoom">
          <label>
            <input type="checkbox" />
            <img
              class="feature__image image--rounded"
              src="/images/gardenhose.png"
              alt="Google Search Results Page"
            />
          </label>
        </div>
      </div>
    </article>

    
    <article class="feature">
      <div class="feature__content">
        
        <h3 class="feature__heading">Google Search Results for &quot;garden hose&quot;. Note the location is set to USA by using the parameter &quot;&amp;gl=us&quot;.</h3>
        

        <p>Next, we must decide how many results we want to scrape. Once again, this can be set by adding a parameter to the end of the URL. In the image below I have added the parameter “&num=50” to ensure that the top 50 results are returned. </p>
      </div>
    </article>
    
    <div class="blog__image__container">
      <div class="click-zoom">
        <label>
          <input type="checkbox" />
          <img
            class="feature__image"
            src="/images/num50.png"
            alt="Google Search Results"
          />
        </label>
      </div>
      
      <p>Google Search Results Page with 50 results instead of the default 10. This is achieved by adding the parameter &quot;&amp;num=50&quot;.</p>
      
    </div>
      
    <article class="feature">
      <div class="feature__content">
        

        <p>Now that we have the desired output for our search term it’s time to work with cookies. </p>



<h2>Step 2: Handling Cookies</h2>

<p>When we run a Google Search query through Python our program will run into an issue with cookies. To bypass this issue, we give our Python program cookies so that Google thinks it is a real user. To find the cookies we need to use we do a quick manual Google Search. On the Google Results Page we can then go Inspect --> Application --> Cookies. Click the cookies for Google and copy the cookie labelled “CONSENT”. This is the cookie we will use in our Python program as a parameter at the time of our request. </p>
      </div>
    </article>
    
    <div class="blog__image__container">
      <div class="click-zoom">
        <label>
          <input type="checkbox" />
          <img
            class="feature__image"
            src="/images/cookies.png"
            alt="Devloper Tools on the Google Results"
          />
        </label>
      </div>
      
      <p>Using Developer Tools we can retrieve the cookies we require from the Application tab.</p>
      
    </div>
      
    <article class="feature">
      <div class="feature__content">
         <p>In the code below we pass our search URL and our Consent cookie to requests and use BeautifulSoup to parse the HTML of the response. </p>

<pre class="code_terminal">

<code>
from bs4 import BeautifulSoup
import requests

urls = ["https://www.google.com/search?q=garden+hose&gl=us&num=50"

cookies = {
    "CONSENT": "YES+cb.20220301-11-p0.en+FX+104"
}

def scrape(url):
    # r = requests.get(url, headers=headers, cookies=cookies)
    r = requests.get(url, cookies=cookies)
    webpage = BeautifulSoup(r.text, "html.parser")

print(webpage)

if name ==  "main":
    for url in urls:
        scrape(url)

</code>

</pre>



<h2>Step 3: Inspecting Returned Page, Not our Browser</h2>

<p>This next step is where a lot of people run into problems scraping Google. Often the page you inspect after conducting a Google Search is not the same HTML which is returned by requests. This means that when you try to target a class in the returned HTML with beautiful soup, it may not exist, and so the program does not return any results. To get around this problem we simply print the HTML response. </p>
      </div>
    </article>
    
    <div class="blog__image__container">
      <div class="click-zoom">
        <label>
          <input type="checkbox" />
          <img
            class="feature__image"
            src="/images/printing-response.png"
            alt="Screenshot of code terminal"
          />
        </label>
      </div>
      
      <p>Printing out the HTML response so we can determine the classes we need to target in our scraping program.</p>
      
    </div>
      
    <article class="feature">
      <div class="feature__content">
         <p>We can then copy this HTML from the terminal and paste it into an editor where we can inspect the code for the attributes we are hoping to target. I like to use a live editor such as <a href="https://htmledit.squarefree.com/" target="_blank">HTML Square Free</a> for this task which renders a live view of the HTML code. We can then open developer tools and look for the classes we need. In the below example we can see that each Google result is enclosed in <div class="egMi0 kCrYT">. </p>
      </div>
    </article>
    
    <div class="blog__image__container">
      <div class="click-zoom">
        <label>
          <input type="checkbox" />
          <img
            class="feature__image"
            src="/images/inspecting-response.png"
            alt="Inspecting the HTML"
          />
        </label>
      </div>
      
      <p>Pasting the HTML response into a live server allows us to inspect the results page for the classes we need to target.  </p>
      
    </div>
      
    <article class="feature">
      <div class="feature__content">
         <p>This means we can target this <div> element to retrieve the URL and Title of the result. The below code does this:</p>\
\
<pre class="code_terminal">

<code>
from bs4 import BeautifulSoup
import requests

urls = ["https://www.google.com/search?q=garden+hose&gl=us&num=50"

cookies = {
    "CONSENT": "YES+cb.20220301-11-p0.en+FX+104"
}

def scrape(url):
    # r = requests.get(url, headers=headers, cookies=cookies)
    r = requests.get(url, cookies=cookies)
    webpage = BeautifulSoup(r.text, "html.parser")

if name ==  "main":
    for url in urls:
        scrape(url)

</code>

</pre>



<p>We can now copy and paste this code to Excel to tidy up.</p>

<h2>Step 4: Using Excel to Highlight Domains of Interest</h2>\
<p>After pasting the data into Excel we can now highlight the column and do a simple find and replace to remove the unnecessary “/url?q=” at the start of each URL.</p>
      </div>
    </article>
    
    <div class="blog__image__container">
      <div class="click-zoom">
        <label>
          <input type="checkbox" />
          <img
            class="feature__image"
            src="/images/find-and-replace.png"
            alt="Excel screenshot"
          />
        </label>
      </div>
      
      <p>In Excel we can use &quot;Find and Replace&quot; to clean up our results.</p>
      
    </div>
      
    <article class="feature">
      <div class="feature__content">
         <p>In the Data tab we can now select Text to Columns --> delimited --> other and use the pipe symbol as a delimiter. Press finish and this should separate out our URLs and Titles in separate columns for analysis.</p>
      </div>
    </article>
    
    <div class="blog__image__container">
      <div class="click-zoom">
        <label>
          <input type="checkbox" />
          <img
            class="feature__image"
            src="/images/text-to-columns.png"
            alt="Text to columns to clean data"
          />
        </label>
      </div>
      
      <p>Using &quot;Text to Columns&quot; we can split our data so that we have both our URLs and Titles in individual columns.</p>
      
    </div>
      
    <article class="feature">
      <div class="feature__content">
         <p>Using conditional formating we can now highlight any domains of interest. To do this on the Home tab click Condtional Formating --> Highlight Cell Rules --> Text that contains and enter the domain you are interested in along with a colour.</p>
      </div>
    </article>
    
    <div class="blog__image__container">
      <div class="click-zoom">
        <label>
          <input type="checkbox" />
          <img
            class="feature__image"
            src="/images/conditional-formatting.png"
            alt="Conditional Formatting in Excel"
          />
        </label>
      </div>
      
      <p>Using Conditional Formatting in Excel we can highlight domains of interest so it is easier to track results.</p>
      
    </div>
      
    <article class="feature">
      <div class="feature__content">
         <p>Lets say we own the domain “yourbestdigs.com” and want to track the ranking of a landing page, plus the ranking of a competitor domain “gardenerspath.com”where they rank. We could colour our domain green and our competitor domain red.</p>\
\
<p>Next we can add some headers and positions to our data. First highlight column A1  right click  insert column. Now select cell B2, navigate to the View tab and select freeze panes. This will freeze the top row and the first column so as we fill up our Excel file the data will be easy to visualise as we scroll. </p>
      </div>
    </article>
    
    <div class="blog__image__container">
      <div class="click-zoom">
        <label>
          <input type="checkbox" />
          <img
            class="feature__image"
            src="/images/freezing-panes.png"
            alt="Freezing panes"
          />
        </label>
      </div>
      
      <p>We can use Excel to &quot;Freeze Panes&quot;. This means that the top row and first column will be locked in position when we scroll, we makes it easier to visualise data as we fill up the Excel file.</p>
      
    </div>
      
    <article class="feature">
      <div class="feature__content">
         <p>We can now fil the numbers down the first column and the date across the top row.</p>
      </div>
    </article>
    
    <div class="blog__image__container">
      <div class="click-zoom">
        <label>
          <input type="checkbox" />
          <img
            class="feature__image"
            src="/images/formatted.png"
            alt="Excel file correctly formatted"
          />
        </label>
      </div>
      
      <p>Adding numbers in the first column denotes the position of the result in Google. We can then track daily changes by adding date to the first row.</p>
      
    </div>
      
    <article class="feature">
      <div class="feature__content">
         <p>Now if we were to rerun the program over a few days we may visualise the position jumps of our domains of interest. </p>
      </div>
    </article>
    
    <div class="blog__image__container">
      <div class="click-zoom">
        <label>
          <input type="checkbox" />
          <img
            class="feature__image"
            src="/images/changes.png"
            alt="Daily changes"
          />
        </label>
      </div>
      
      <p>If we rerun the program over a time period we cant rack the changes in Google Results. In this example we can see our domain has climbed 3 positions while our competitor has dropped 1.</p>
      
    </div>
      
    <article class="feature">
      <div class="feature__content">
         <h2>Full Code</h2>

<pre class="code_terminal">

<code>
from bs4 import BeautifulSoup
import requests

urls = ["https://www.google.com/search?q=search+term +here&gl=location here&num=number of results here"

cookies = {
    "CONSENT": "Your cookie here"
}

def scrape(url):
    # r = requests.get(url, headers=headers, cookies=cookies)
    r = requests.get(url, cookies=cookies)
    webpage = BeautifulSoup(r.text, "html.parser")

if name ==  "main":
    for url in urls:
        scrape(url)

</code>

</pre>
      </div>
    </article>
        
  </div>
</section>

<section data-aos="fade-up" class="block container">
  <header class="block__header">
    <h3>Blog by Category</h3>
  </header>

  <div class="center">
  <ul class="center_list">
    <li>
      <a href="/featured-blogs/">Recent</a>
    </li>
    <li>
      <a href="/coding-blogs/">Coding</a>
    </li>
    <!-- <li>
      <a href="/DIY-blogs/">DIY</a>
    </li>
    <li>
      <a href="/outdoors-blogs/">Outdoors</a>
    </li> -->
  </ul>
</div>

</section>

<script src="js/main.js"></script>
<script src="https://unpkg.com/aos@next/dist/aos.js"></script>
<script>
  AOS.init();
</script>

    </main>
    <script src="https://unpkg.com/aos@next/dist/aos.js"></script>
<script>
  AOS.init();
</script>
    <footer><footer class="block block--dark footer">
  <div class="container grid footer__sections">
    <section class="collapsible collapsible--expanded footer__section">
      <header class="collapsible__header">
        <h2 class="collapsible__heading footer__heading">Sean's Guide</h2>
        <svg class="icon icon--white collapsible-icon">
          <use href="images/sprite.svg#chevron"></use>
        </svg>
      </header>
      <div class="collapsible__content">
        <ul class="list">
          <li><a href="/">Home</a></li>
          <li><a href="/blog/">Blog</a></li>
          <li><a href="/contact/">Contact</a></li>
        </ul>
      </div>
    </section>

    <section class="footer__brand">
      <a class="nav__brand" href="index.html">
        <img src="/images/logo2.jpg" alt="Brand logo"
      /></a>
      <p class="footer__year">2022</p>
    </section>
  </div>
</footer>
</footer>
    <script src="/js/main.js"></script>
    <script>
      if (window.netlifyIdentity) {
        window.netlifyIdentity.on("init", (user) => {
          if (!user) {
            window.netlifyIdentity.on("login", () => {
              document.location.href = "/admin/";
            });
          }
        });
      }
    </script>
  </body>
</html>
